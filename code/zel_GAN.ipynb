{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Tensorflow\n",
    "import tensorflow as tf\n",
    "# Image data and layers pacakges\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Dense, Flatten, Reshape, LeakyReLU, Dropout, UpSampling2D, MaxPool2D, Rescaling\n",
    "from tensorflow.keras.layers import RandomFlip, RandomRotation, RandomZoom\n",
    "# Losses and Optimizers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "\n",
    "# Base model class to subclass for training step\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Load images from directory\n",
    "from tensorflow.keras.utils import image_dataset_from_directory\n",
    "# For callbacks and checkpoints\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.train import Checkpoint\n",
    "# Extra\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "os.chdir(\"C:\\\\GANs\\\\zelda_gan\\\\code\")\n",
    "resize_path = \"..\\\\resized_images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "img_height = 40\n",
    "img_width = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def load_images(in_dir, img_height, img_width, batch_size, save_dir):\n",
    "    norm = lambda x: x.astype(\"float32\")/255\n",
    "    dg = ImageDataGenerator(preprocessing_function = img_norm, zoom_range = 0.25, \n",
    "                                  horizontal_flip = True, rotation_range = 0.05)\n",
    "    x_train = dg.flow_from_directory(in_dir,\n",
    "                                            target_size = (img_height, img_width),\n",
    "                                            batch_size = batch_size,\n",
    "                                            shuffle = True,\n",
    "                                            save_to_dir = save_dir,\n",
    "                                            classes = None,\n",
    "                                            class_mode = None,\n",
    "                                            subset = \"training\")\n",
    "    return x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2340 files belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "img_norm = tf.keras.layers.Rescaling(1./255)\n",
    "train_ds = image_dataset_from_directory(resize_path, \n",
    "                                       labels = None, \n",
    "                                       color_mode = 'grayscale', \n",
    "                                       image_size = (img_height , img_width),\n",
    "                                       batch_size = batch_size, \n",
    "                                       shuffle = True)\n",
    "train_ds = train_ds.map(img_norm)\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generative NN\n",
    "def build_gen_nn():\n",
    "    # Initialize Model\n",
    "    model = Sequential()\n",
    "\n",
    "    # Takes in random values and reshape it to 7x7x64\n",
    "    model.add(Dense(20*15*128, input_dim = 64))\n",
    "    model.add(LeakyReLU(.2))\n",
    "    model.add(Reshape((20, 15, 128)))\n",
    "\n",
    "    # Effectively doubles the size of previous layer\n",
    "    model.add(UpSampling2D())\n",
    "    model.add(Conv2D(128, 5, padding = \"same\"))\n",
    "    model.add(LeakyReLU(.2))\n",
    "\n",
    "    # Second Upsampling block\n",
    "    model.add(UpSampling2D())\n",
    "    model.add(Conv2D(256, 5, padding = \"same\"))\n",
    "    model.add(LeakyReLU(.2))\n",
    "    \n",
    "        \n",
    "    # Down Sample\n",
    "    model.add(MaxPool2D())\n",
    "    model.add(Conv2D(256, 5, padding = \"same\"))\n",
    "    model.add(LeakyReLU(.2))\n",
    "    \n",
    "    # Normal Convolutional Block\n",
    "    model.add(Conv2D(256, 4, padding = \"same\"))\n",
    "    model.add(LeakyReLU(.2))\n",
    "    \n",
    "    # Normal Convolutional Block\n",
    "    model.add(Conv2D(512, 4, padding = \"same\"))\n",
    "    model.add(LeakyReLU(.2))\n",
    "    \n",
    "    # Second Convolutional Block\n",
    "    model.add(Conv2D(512, 4, padding = \"same\"))\n",
    "    model.add(LeakyReLU(.2))\n",
    "\n",
    "\n",
    "    # Conv layer to get to one channel\n",
    "    model.add(Conv2D(1, 4, padding = 'same', activation = \"sigmoid\"))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def build_disc_nn():\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(32, 5, input_shape = (40,30,1)))\n",
    "    model.add(LeakyReLU(.2))\n",
    "    model.add(Dropout(.2))\n",
    "\n",
    "    model.add(Conv2D(64, 5))\n",
    "    model.add(LeakyReLU(.2))\n",
    "    model.add(Dropout(.2))\n",
    "\n",
    "    model.add(Conv2D(128, 5))\n",
    "    model.add(LeakyReLU(.2))\n",
    "    model.add(Dropout(.2))\n",
    "\n",
    "    model.add(Conv2D(256, 5))\n",
    "    model.add(LeakyReLU(.2))\n",
    "    model.add(Dropout(.2))\n",
    "    \n",
    "    model.add(Conv2D(512, 5))\n",
    "    model.add(LeakyReLU(.2))\n",
    "    model.add(Dropout(.2))\n",
    "\n",
    "    model.add(Conv2D(512, 5))\n",
    "    model.add(LeakyReLU(.2))\n",
    "    model.add(Dropout(.2))\n",
    "\n",
    "    # Flatten then pass to a dense layer\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(.4))\n",
    "    model.add(Dense(1, activation = \"sigmoid\"))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class ZelGAN(Model):\n",
    "    def __init__(self, generator, discriminator, *args, **kwargs):\n",
    "        # pass args and kwards to base class\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "        # Create attributes for two models\n",
    "        self.generator = generator\n",
    "        self.discriminator = discriminator\n",
    "        self.transformation = Sequential([\n",
    "  RandomFlip(\"horizontal_and_vertical\"),\n",
    "  RandomRotation(0.2),\n",
    "  RandomZoom(height_factor=(-0.1, 0.1), width_factor = (-0.1, 0.1))\n",
    "])\n",
    "\n",
    "        pass\n",
    "    \n",
    "    def compile(self, gen_opt, disc_opt, gen_loss, disc_loss, *args, **kwargs):\n",
    "        # Compile with base class\n",
    "        super().compile(*args, **kwargs)\n",
    "\n",
    "        # Create attributes for losses and optimizers\n",
    "        self.gen_opt = gen_opt\n",
    "        self.disc_opt = disc_opt\n",
    "        self.gen_loss = gen_loss\n",
    "        self.disc_loss = disc_loss\n",
    "    \n",
    "    def train_step(self, batch):\n",
    "        # Get the data\n",
    "        real_images = self.transformation(batch)\n",
    "        gen_images = self.generator(tf.random.normal((32, 64, 1), 0, 2), training = False)\n",
    "        # Train the discriminator\n",
    "        with tf.GradientTape() as disc_tape:\n",
    "            # Pass the real and generated images to the discriminator\n",
    "            yhat_real = self.discriminator(real_images, training = True)\n",
    "            yhat_gen = self.discriminator(gen_images, training = True)\n",
    "            yhat_all = tf.concat([yhat_real, yhat_gen], axis = 0)\n",
    "\n",
    "            # Creat labels for real and generated images\n",
    "            y_all = tf.concat([tf.zeros_like(yhat_real), tf.ones_like(yhat_gen)], axis = 0)\n",
    "\n",
    "            # Add some noise to the TRUE outputs\n",
    "            noise_real = .1 * tf.random.normal(tf.shape(yhat_real), 0, 2)\n",
    "            noise_gen = -.1 * tf.random.normal(tf.shape(yhat_gen), 0, 2)\n",
    "            y_all += tf.concat([noise_real, noise_gen], axis = 0)\n",
    "\n",
    "            # Calculate loss\n",
    "            total_disc_loss = self.disc_loss(y_all, yhat_all)\n",
    "\n",
    "        # Apply backprop\n",
    "        disc_grad = disc_tape.gradient(total_disc_loss, self.discriminator.trainable_variables)\n",
    "        self.disc_opt.apply_gradients(zip(disc_grad, self.discriminator.trainable_variables))\n",
    "\n",
    "        with tf.GradientTape() as gen_tape:\n",
    "            # Generate some new images\n",
    "            gen_images = self.generator(tf.random.normal((64, 64, 1), 0, 2), training = True)\n",
    "            \n",
    "            # Create the predicted labels\n",
    "            predicted_labels = self.discriminator(gen_images, training = False)\n",
    "            \n",
    "            # Calculate Loss\n",
    "            total_gen_loss = self.gen_loss(tf.zeros_like(predicted_labels), predicted_labels)\n",
    "            \n",
    "        # Apply Backprop\n",
    "        gen_grad = gen_tape.gradient(total_gen_loss, self.generator.trainable_variables)\n",
    "        self.gen_opt.apply_gradients(zip(gen_grad, self.generator.trainable_variables))\n",
    "        \n",
    "        \n",
    "\n",
    "        return {\"disc_loss\" : total_disc_loss, \"gen_loss\" : total_gen_loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize Networks\n",
    "generator = build_gen_nn()\n",
    "discriminator = build_disc_nn()\n",
    "\n",
    "# Set up Losses and Optimizers\n",
    "gen_opt = Adam(learning_rate = .000005, beta_1=0.8, clipvalue=1.0)\n",
    "disc_opt = Adam(learning_rate = .0000005, beta_1=0.8, clipvalue=1.0)\n",
    "gen_loss = BinaryCrossentropy()\n",
    "disc_loss = BinaryCrossentropy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Zelda = ZelGAN(generator, discriminator)\n",
    "Zelda.compile(gen_opt, disc_opt, gen_loss, disc_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class ModelMonitor(Callback):\n",
    "    def __init__(self, num_img = 1, latent_dim = 64):\n",
    "        self.num_img = num_img\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs = None):\n",
    "        random_latent_vectors = tf.random.normal((self.num_img, self.latent_dim, 1), 0, 2)\n",
    "        generated_images = self.model.generator(random_latent_vectors)\n",
    "        generated_images *= 255\n",
    "        generated_images.numpy()\n",
    "        for i in range(self.num_img):\n",
    "            img = array_to_img(generated_images[i])\n",
    "            img.save(os.path.join(\"..\\\\progress_images\", f\"generated_img_{epoch}_{i}.png\"))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Zelda.generator.load_weights('..//saves//generator_norm.h5')\n",
    "Zelda.discriminator.load_weights('..//saves//discriminator_norm.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/512\n",
      "74/74 [==============================] - 93s 1s/step - disc_loss: 0.6873 - gen_loss: 0.6616\n",
      "Epoch 2/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6890 - gen_loss: 0.6608\n",
      "Epoch 3/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6887 - gen_loss: 0.6599\n",
      "Epoch 4/512\n",
      "74/74 [==============================] - 84s 1s/step - disc_loss: 0.6909 - gen_loss: 0.6594\n",
      "Epoch 5/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6896 - gen_loss: 0.6587\n",
      "Epoch 6/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6898 - gen_loss: 0.6580\n",
      "Epoch 7/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6898 - gen_loss: 0.6572\n",
      "Epoch 8/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6887 - gen_loss: 0.6566\n",
      "Epoch 9/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6907 - gen_loss: 0.6561\n",
      "Epoch 10/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6896 - gen_loss: 0.6554\n",
      "Epoch 11/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6895 - gen_loss: 0.6546\n",
      "Epoch 12/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6899 - gen_loss: 0.6542\n",
      "Epoch 13/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6894 - gen_loss: 0.6533\n",
      "Epoch 14/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6915 - gen_loss: 0.6529\n",
      "Epoch 15/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6907 - gen_loss: 0.6521\n",
      "Epoch 16/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6904 - gen_loss: 0.6512\n",
      "Epoch 17/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6918 - gen_loss: 0.6504\n",
      "Epoch 18/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6908 - gen_loss: 0.6510\n",
      "Epoch 19/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6923 - gen_loss: 0.6494\n",
      "Epoch 20/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6911 - gen_loss: 0.6484\n",
      "Epoch 21/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6908 - gen_loss: 0.6472\n",
      "Epoch 22/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6904 - gen_loss: 0.6460\n",
      "Epoch 23/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6915 - gen_loss: 0.6450\n",
      "Epoch 24/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6922 - gen_loss: 0.6442\n",
      "Epoch 25/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6930 - gen_loss: 0.6434\n",
      "Epoch 26/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6914 - gen_loss: 0.6421\n",
      "Epoch 27/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6920 - gen_loss: 0.6412\n",
      "Epoch 28/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6937 - gen_loss: 0.6407\n",
      "Epoch 29/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6923 - gen_loss: 0.6392\n",
      "Epoch 30/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6926 - gen_loss: 0.6380\n",
      "Epoch 31/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6946 - gen_loss: 0.6371\n",
      "Epoch 32/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6938 - gen_loss: 0.6369\n",
      "Epoch 33/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6944 - gen_loss: 0.6359\n",
      "Epoch 34/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6933 - gen_loss: 0.6342\n",
      "Epoch 35/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6935 - gen_loss: 0.6335\n",
      "Epoch 36/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6962 - gen_loss: 0.6333\n",
      "Epoch 37/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6954 - gen_loss: 0.6326\n",
      "Epoch 38/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6953 - gen_loss: 0.6318\n",
      "Epoch 39/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6963 - gen_loss: 0.6318\n",
      "Epoch 40/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6967 - gen_loss: 0.6316\n",
      "Epoch 41/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6963 - gen_loss: 0.6308\n",
      "Epoch 42/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6969 - gen_loss: 0.6307\n",
      "Epoch 43/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6969 - gen_loss: 0.6303\n",
      "Epoch 44/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6971 - gen_loss: 0.6297\n",
      "Epoch 45/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6975 - gen_loss: 0.6301\n",
      "Epoch 46/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6971 - gen_loss: 0.6295\n",
      "Epoch 47/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6974 - gen_loss: 0.6295\n",
      "Epoch 48/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6972 - gen_loss: 0.6292\n",
      "Epoch 49/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6981 - gen_loss: 0.6293\n",
      "Epoch 50/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6978 - gen_loss: 0.6291\n",
      "Epoch 51/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6983 - gen_loss: 0.6292\n",
      "Epoch 52/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6986 - gen_loss: 0.6299\n",
      "Epoch 53/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6984 - gen_loss: 0.6293\n",
      "Epoch 54/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6986 - gen_loss: 0.6306\n",
      "Epoch 55/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6988 - gen_loss: 0.6310\n",
      "Epoch 56/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6983 - gen_loss: 0.6311\n",
      "Epoch 57/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6982 - gen_loss: 0.6318\n",
      "Epoch 58/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6975 - gen_loss: 0.6324\n",
      "Epoch 59/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6980 - gen_loss: 0.6336\n",
      "Epoch 60/512\n",
      "74/74 [==============================] - 83s 1s/step - disc_loss: 0.6974 - gen_loss: 0.6341\n",
      "Epoch 61/512\n",
      "74/74 [==============================] - 83s 1s/step - disc_loss: 0.6977 - gen_loss: 0.6356\n",
      "Epoch 62/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6969 - gen_loss: 0.6360\n",
      "Epoch 63/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6972 - gen_loss: 0.6376\n",
      "Epoch 64/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6971 - gen_loss: 0.6388\n",
      "Epoch 65/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6971 - gen_loss: 0.6404\n",
      "Epoch 66/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6966 - gen_loss: 0.6415\n",
      "Epoch 67/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6966 - gen_loss: 0.6425\n",
      "Epoch 68/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6966 - gen_loss: 0.6436\n",
      "Epoch 69/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6971 - gen_loss: 0.6453\n",
      "Epoch 70/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6966 - gen_loss: 0.6464\n",
      "Epoch 71/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6962 - gen_loss: 0.6473\n",
      "Epoch 72/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6963 - gen_loss: 0.6486\n",
      "Epoch 73/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6960 - gen_loss: 0.6497\n",
      "Epoch 74/512\n",
      "74/74 [==============================] - 83s 1s/step - disc_loss: 0.6962 - gen_loss: 0.6510\n",
      "Epoch 75/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6957 - gen_loss: 0.6511\n",
      "Epoch 76/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6953 - gen_loss: 0.6523\n",
      "Epoch 77/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6954 - gen_loss: 0.6530\n",
      "Epoch 78/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6961 - gen_loss: 0.6541\n",
      "Epoch 79/512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6957 - gen_loss: 0.6553\n",
      "Epoch 80/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6953 - gen_loss: 0.6562\n",
      "Epoch 81/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6954 - gen_loss: 0.6567\n",
      "Epoch 82/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6946 - gen_loss: 0.6571\n",
      "Epoch 83/512\n",
      "74/74 [==============================] - 83s 1s/step - disc_loss: 0.6948 - gen_loss: 0.6578\n",
      "Epoch 84/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6951 - gen_loss: 0.6584\n",
      "Epoch 85/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6951 - gen_loss: 0.6594\n",
      "Epoch 86/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6952 - gen_loss: 0.6596\n",
      "Epoch 87/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6947 - gen_loss: 0.6603\n",
      "Epoch 88/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6947 - gen_loss: 0.6620\n",
      "Epoch 89/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6953 - gen_loss: 0.6621\n",
      "Epoch 90/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6947 - gen_loss: 0.6620\n",
      "Epoch 91/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6948 - gen_loss: 0.6625\n",
      "Epoch 92/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6946 - gen_loss: 0.6632\n",
      "Epoch 93/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6945 - gen_loss: 0.6642\n",
      "Epoch 94/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6945 - gen_loss: 0.6644\n",
      "Epoch 95/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6945 - gen_loss: 0.6652\n",
      "Epoch 96/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6945 - gen_loss: 0.6650\n",
      "Epoch 97/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6944 - gen_loss: 0.6659\n",
      "Epoch 98/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6944 - gen_loss: 0.6657\n",
      "Epoch 99/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6941 - gen_loss: 0.6662\n",
      "Epoch 100/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6946 - gen_loss: 0.6672\n",
      "Epoch 101/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6942 - gen_loss: 0.6672\n",
      "Epoch 102/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6943 - gen_loss: 0.6674\n",
      "Epoch 103/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6943 - gen_loss: 0.6676\n",
      "Epoch 104/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6940 - gen_loss: 0.6686\n",
      "Epoch 105/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6945 - gen_loss: 0.6689\n",
      "Epoch 106/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6943 - gen_loss: 0.6684\n",
      "Epoch 107/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6942 - gen_loss: 0.6684\n",
      "Epoch 108/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6943 - gen_loss: 0.6695\n",
      "Epoch 109/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6941 - gen_loss: 0.6688\n",
      "Epoch 110/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6938 - gen_loss: 0.6693\n",
      "Epoch 111/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6939 - gen_loss: 0.6705\n",
      "Epoch 112/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6941 - gen_loss: 0.6709\n",
      "Epoch 113/512\n",
      "74/74 [==============================] - 83s 1s/step - disc_loss: 0.6938 - gen_loss: 0.6714\n",
      "Epoch 114/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6942 - gen_loss: 0.6713\n",
      "Epoch 115/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6937 - gen_loss: 0.6715\n",
      "Epoch 116/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6934 - gen_loss: 0.6719\n",
      "Epoch 117/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6941 - gen_loss: 0.6724\n",
      "Epoch 118/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6942 - gen_loss: 0.6721\n",
      "Epoch 119/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6937 - gen_loss: 0.6727\n",
      "Epoch 120/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6938 - gen_loss: 0.6725\n",
      "Epoch 121/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6938 - gen_loss: 0.6726\n",
      "Epoch 122/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6941 - gen_loss: 0.6730\n",
      "Epoch 123/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6938 - gen_loss: 0.6733\n",
      "Epoch 124/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6934 - gen_loss: 0.6735\n",
      "Epoch 125/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6935 - gen_loss: 0.6734\n",
      "Epoch 126/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6936 - gen_loss: 0.6743\n",
      "Epoch 127/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6937 - gen_loss: 0.6735\n",
      "Epoch 128/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6938 - gen_loss: 0.6735\n",
      "Epoch 129/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6936 - gen_loss: 0.6743\n",
      "Epoch 130/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6940 - gen_loss: 0.6743\n",
      "Epoch 131/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6940 - gen_loss: 0.6745\n",
      "Epoch 132/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6935 - gen_loss: 0.6746\n",
      "Epoch 133/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6934 - gen_loss: 0.6753\n",
      "Epoch 134/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6930 - gen_loss: 0.6759\n",
      "Epoch 135/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6931 - gen_loss: 0.6766\n",
      "Epoch 136/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6926 - gen_loss: 0.6765\n",
      "Epoch 137/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6929 - gen_loss: 0.6764\n",
      "Epoch 138/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6925 - gen_loss: 0.6765\n",
      "Epoch 139/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6922 - gen_loss: 0.6766\n",
      "Epoch 140/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6919 - gen_loss: 0.6769\n",
      "Epoch 141/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6919 - gen_loss: 0.6770\n",
      "Epoch 142/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6921 - gen_loss: 0.6772\n",
      "Epoch 143/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6926 - gen_loss: 0.6774\n",
      "Epoch 144/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6915 - gen_loss: 0.6770\n",
      "Epoch 145/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6911 - gen_loss: 0.6774\n",
      "Epoch 146/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6902 - gen_loss: 0.6773\n",
      "Epoch 147/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6904 - gen_loss: 0.6775\n",
      "Epoch 148/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6892 - gen_loss: 0.6774\n",
      "Epoch 149/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6876 - gen_loss: 0.6773\n",
      "Epoch 150/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6881 - gen_loss: 0.6769\n",
      "Epoch 151/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6856 - gen_loss: 0.6762\n",
      "Epoch 152/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6861 - gen_loss: 0.6748\n",
      "Epoch 153/512\n",
      "74/74 [==============================] - 83s 1s/step - disc_loss: 0.6858 - gen_loss: 0.6747\n",
      "Epoch 154/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6920 - gen_loss: 0.6750\n",
      "Epoch 155/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6876 - gen_loss: 0.6736\n",
      "Epoch 156/512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6860 - gen_loss: 0.6735\n",
      "Epoch 157/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6845 - gen_loss: 0.6734\n",
      "Epoch 158/512\n",
      "74/74 [==============================] - 83s 1s/step - disc_loss: 0.6823 - gen_loss: 0.6738\n",
      "Epoch 159/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6794 - gen_loss: 0.6740\n",
      "Epoch 160/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6771 - gen_loss: 0.6730\n",
      "Epoch 161/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6729 - gen_loss: 0.6715\n",
      "Epoch 162/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6718 - gen_loss: 0.6701\n",
      "Epoch 163/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6681 - gen_loss: 0.6681\n",
      "Epoch 164/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6648 - gen_loss: 0.6656\n",
      "Epoch 165/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6595 - gen_loss: 0.6621\n",
      "Epoch 166/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6573 - gen_loss: 0.6583\n",
      "Epoch 167/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6556 - gen_loss: 0.6538\n",
      "Epoch 168/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6519 - gen_loss: 0.6476\n",
      "Epoch 169/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6514 - gen_loss: 0.6417\n",
      "Epoch 170/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6480 - gen_loss: 0.6341\n",
      "Epoch 171/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6469 - gen_loss: 0.6255\n",
      "Epoch 172/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6453 - gen_loss: 0.6165\n",
      "Epoch 173/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6425 - gen_loss: 0.6064\n",
      "Epoch 174/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6420 - gen_loss: 0.5946\n",
      "Epoch 175/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6426 - gen_loss: 0.5823\n",
      "Epoch 176/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6525 - gen_loss: 0.5720\n",
      "Epoch 177/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6500 - gen_loss: 0.5618\n",
      "Epoch 178/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6493 - gen_loss: 0.5531\n",
      "Epoch 179/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6545 - gen_loss: 0.5431\n",
      "Epoch 180/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6607 - gen_loss: 0.5370\n",
      "Epoch 181/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6554 - gen_loss: 0.5303\n",
      "Epoch 182/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6577 - gen_loss: 0.5214\n",
      "Epoch 183/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6655 - gen_loss: 0.5165\n",
      "Epoch 184/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6640 - gen_loss: 0.5096\n",
      "Epoch 185/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6614 - gen_loss: 0.5045\n",
      "Epoch 186/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6681 - gen_loss: 0.4975\n",
      "Epoch 187/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6639 - gen_loss: 0.4905\n",
      "Epoch 188/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6731 - gen_loss: 0.4864\n",
      "Epoch 189/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6693 - gen_loss: 0.4800\n",
      "Epoch 190/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6708 - gen_loss: 0.4721\n",
      "Epoch 191/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6759 - gen_loss: 0.4642\n",
      "Epoch 192/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6844 - gen_loss: 0.4621\n",
      "Epoch 193/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6845 - gen_loss: 0.4562\n",
      "Epoch 194/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6933 - gen_loss: 0.4501\n",
      "Epoch 195/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6990 - gen_loss: 0.4459\n",
      "Epoch 196/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.7105 - gen_loss: 0.4434\n",
      "Epoch 197/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.7135 - gen_loss: 0.4422\n",
      "Epoch 198/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.7181 - gen_loss: 0.4436\n",
      "Epoch 199/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.7153 - gen_loss: 0.4445\n",
      "Epoch 200/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.7199 - gen_loss: 0.4485\n",
      "Epoch 201/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.7156 - gen_loss: 0.4508\n",
      "Epoch 202/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.7129 - gen_loss: 0.4537\n",
      "Epoch 203/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.7107 - gen_loss: 0.4571\n",
      "Epoch 204/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.7048 - gen_loss: 0.4590\n",
      "Epoch 205/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.7060 - gen_loss: 0.4638\n",
      "Epoch 206/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.7013 - gen_loss: 0.4660\n",
      "Epoch 207/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6937 - gen_loss: 0.4669\n",
      "Epoch 208/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6988 - gen_loss: 0.4673\n",
      "Epoch 209/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.7030 - gen_loss: 0.4689\n",
      "Epoch 210/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.7054 - gen_loss: 0.4721\n",
      "Epoch 211/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.7071 - gen_loss: 0.4743\n",
      "Epoch 212/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.7127 - gen_loss: 0.4795\n",
      "Epoch 213/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.7162 - gen_loss: 0.4860\n",
      "Epoch 214/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.7197 - gen_loss: 0.4934\n",
      "Epoch 215/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.7213 - gen_loss: 0.4993\n",
      "Epoch 216/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.7233 - gen_loss: 0.5089\n",
      "Epoch 217/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.7254 - gen_loss: 0.5180\n",
      "Epoch 218/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.7235 - gen_loss: 0.5265\n",
      "Epoch 219/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.7198 - gen_loss: 0.5374\n",
      "Epoch 220/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.7147 - gen_loss: 0.5472\n",
      "Epoch 221/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.7118 - gen_loss: 0.5586\n",
      "Epoch 222/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.7085 - gen_loss: 0.5717\n",
      "Epoch 223/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.7059 - gen_loss: 0.5847\n",
      "Epoch 224/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.7049 - gen_loss: 0.5933\n",
      "Epoch 225/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.7049 - gen_loss: 0.6065\n",
      "Epoch 226/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.7024 - gen_loss: 0.6088\n",
      "Epoch 227/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.7031 - gen_loss: 0.6138\n",
      "Epoch 228/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.7006 - gen_loss: 0.6191\n",
      "Epoch 229/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.7013 - gen_loss: 0.6211\n",
      "Epoch 230/512\n",
      "74/74 [==============================] - 83s 1s/step - disc_loss: 0.7009 - gen_loss: 0.6277\n",
      "Epoch 231/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6999 - gen_loss: 0.6287\n",
      "Epoch 232/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6986 - gen_loss: 0.6301\n",
      "Epoch 233/512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74/74 [==============================] - 83s 1s/step - disc_loss: 0.6994 - gen_loss: 0.6313\n",
      "Epoch 234/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6996 - gen_loss: 0.6366\n",
      "Epoch 235/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6979 - gen_loss: 0.6381\n",
      "Epoch 236/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6984 - gen_loss: 0.6394\n",
      "Epoch 237/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6986 - gen_loss: 0.6379\n",
      "Epoch 238/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6969 - gen_loss: 0.6416\n",
      "Epoch 239/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6993 - gen_loss: 0.6435\n",
      "Epoch 240/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6981 - gen_loss: 0.6452\n",
      "Epoch 241/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6982 - gen_loss: 0.6460\n",
      "Epoch 242/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6962 - gen_loss: 0.6492\n",
      "Epoch 243/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6974 - gen_loss: 0.6486\n",
      "Epoch 244/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6966 - gen_loss: 0.6525\n",
      "Epoch 245/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6975 - gen_loss: 0.6483\n",
      "Epoch 246/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6971 - gen_loss: 0.6515\n",
      "Epoch 247/512\n",
      "74/74 [==============================] - 83s 1s/step - disc_loss: 0.6971 - gen_loss: 0.6518\n",
      "Epoch 248/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6960 - gen_loss: 0.6537\n",
      "Epoch 249/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6965 - gen_loss: 0.6527\n",
      "Epoch 250/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6975 - gen_loss: 0.6548\n",
      "Epoch 251/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6960 - gen_loss: 0.6551\n",
      "Epoch 252/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6965 - gen_loss: 0.6548\n",
      "Epoch 253/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6962 - gen_loss: 0.6564\n",
      "Epoch 254/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6949 - gen_loss: 0.6553\n",
      "Epoch 255/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6934 - gen_loss: 0.6557\n",
      "Epoch 256/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6946 - gen_loss: 0.6571\n",
      "Epoch 257/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6958 - gen_loss: 0.6566\n",
      "Epoch 258/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6939 - gen_loss: 0.6572\n",
      "Epoch 259/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6945 - gen_loss: 0.6576\n",
      "Epoch 260/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6952 - gen_loss: 0.6582\n",
      "Epoch 261/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6949 - gen_loss: 0.6586\n",
      "Epoch 262/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6962 - gen_loss: 0.6601\n",
      "Epoch 263/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6946 - gen_loss: 0.6605\n",
      "Epoch 264/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6945 - gen_loss: 0.6612\n",
      "Epoch 265/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6935 - gen_loss: 0.6614\n",
      "Epoch 266/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6947 - gen_loss: 0.6621\n",
      "Epoch 267/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6948 - gen_loss: 0.6621\n",
      "Epoch 268/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6952 - gen_loss: 0.6634\n",
      "Epoch 269/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6948 - gen_loss: 0.6639\n",
      "Epoch 270/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6946 - gen_loss: 0.6642\n",
      "Epoch 271/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6944 - gen_loss: 0.6643\n",
      "Epoch 272/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6938 - gen_loss: 0.6640\n",
      "Epoch 273/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6946 - gen_loss: 0.6642\n",
      "Epoch 274/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6943 - gen_loss: 0.6652\n",
      "Epoch 275/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6938 - gen_loss: 0.6649\n",
      "Epoch 276/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6944 - gen_loss: 0.6653\n",
      "Epoch 277/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6946 - gen_loss: 0.6661\n",
      "Epoch 278/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6947 - gen_loss: 0.6666\n",
      "Epoch 279/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6938 - gen_loss: 0.6678\n",
      "Epoch 280/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6937 - gen_loss: 0.6673\n",
      "Epoch 281/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6944 - gen_loss: 0.6676\n",
      "Epoch 282/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6941 - gen_loss: 0.6675\n",
      "Epoch 283/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6941 - gen_loss: 0.6664\n",
      "Epoch 284/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6936 - gen_loss: 0.6670\n",
      "Epoch 285/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6935 - gen_loss: 0.6666\n",
      "Epoch 286/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6923 - gen_loss: 0.6654\n",
      "Epoch 287/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6933 - gen_loss: 0.6660\n",
      "Epoch 288/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6939 - gen_loss: 0.6654\n",
      "Epoch 289/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6937 - gen_loss: 0.6654\n",
      "Epoch 290/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6935 - gen_loss: 0.6661\n",
      "Epoch 291/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6943 - gen_loss: 0.6651\n",
      "Epoch 292/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6938 - gen_loss: 0.6641\n",
      "Epoch 293/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6921 - gen_loss: 0.6646\n",
      "Epoch 294/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6930 - gen_loss: 0.6643\n",
      "Epoch 295/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6913 - gen_loss: 0.6642\n",
      "Epoch 296/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6913 - gen_loss: 0.6644\n",
      "Epoch 297/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6881 - gen_loss: 0.6625\n",
      "Epoch 298/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6812 - gen_loss: 0.6603\n",
      "Epoch 299/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6574 - gen_loss: 0.6524\n",
      "Epoch 300/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6335 - gen_loss: 0.6312\n",
      "Epoch 301/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6446 - gen_loss: 0.6027\n",
      "Epoch 302/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6641 - gen_loss: 0.5788\n",
      "Epoch 303/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6929 - gen_loss: 0.5728\n",
      "Epoch 304/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6917 - gen_loss: 0.5740\n",
      "Epoch 305/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6934 - gen_loss: 0.5709\n",
      "Epoch 306/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6956 - gen_loss: 0.5683\n",
      "Epoch 307/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6923 - gen_loss: 0.5637\n",
      "Epoch 308/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6901 - gen_loss: 0.5560\n",
      "Epoch 309/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6964 - gen_loss: 0.5530\n",
      "Epoch 310/512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.7030 - gen_loss: 0.5566\n",
      "Epoch 311/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.7028 - gen_loss: 0.5614\n",
      "Epoch 312/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.7031 - gen_loss: 0.5691\n",
      "Epoch 313/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.7006 - gen_loss: 0.5756\n",
      "Epoch 314/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.7023 - gen_loss: 0.5829\n",
      "Epoch 315/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6999 - gen_loss: 0.5923\n",
      "Epoch 316/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6990 - gen_loss: 0.5985\n",
      "Epoch 317/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6980 - gen_loss: 0.6040\n",
      "Epoch 318/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6949 - gen_loss: 0.6094\n",
      "Epoch 319/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6944 - gen_loss: 0.6128\n",
      "Epoch 320/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6980 - gen_loss: 0.6162\n",
      "Epoch 321/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6939 - gen_loss: 0.6234\n",
      "Epoch 322/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6920 - gen_loss: 0.6273\n",
      "Epoch 323/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6906 - gen_loss: 0.6291\n",
      "Epoch 324/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6920 - gen_loss: 0.6302\n",
      "Epoch 325/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6936 - gen_loss: 0.6410\n",
      "Epoch 326/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6915 - gen_loss: 0.6456\n",
      "Epoch 327/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6300 - gen_loss: 0.8511\n",
      "Epoch 328/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6548 - gen_loss: 0.8530\n",
      "Epoch 329/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6515 - gen_loss: 0.6765\n",
      "Epoch 330/512\n",
      "74/74 [==============================] - 81s 1s/step - disc_loss: 0.5908 - gen_loss: 0.5996\n",
      "Epoch 331/512\n",
      "74/74 [==============================] - 81s 1s/step - disc_loss: 0.4682 - gen_loss: 0.6911\n",
      "Epoch 332/512\n",
      "74/74 [==============================] - 80s 1s/step - disc_loss: 0.3822 - gen_loss: 0.7364\n",
      "Epoch 333/512\n",
      "74/74 [==============================] - 81s 1s/step - disc_loss: 0.2903 - gen_loss: 0.8625\n",
      "Epoch 334/512\n",
      "74/74 [==============================] - 81s 1s/step - disc_loss: 0.1966 - gen_loss: 1.0894\n",
      "Epoch 335/512\n",
      "74/74 [==============================] - 80s 1s/step - disc_loss: 0.1512 - gen_loss: 1.3492\n",
      "Epoch 336/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.5992 - gen_loss: 0.1893\n",
      "Epoch 337/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.5485 - gen_loss: 0.1115\n",
      "Epoch 338/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.5303 - gen_loss: 0.0917\n",
      "Epoch 339/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.5661 - gen_loss: 0.0811\n",
      "Epoch 340/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6879 - gen_loss: 0.0853\n",
      "Epoch 341/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.8009 - gen_loss: 0.1030\n",
      "Epoch 342/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.8579 - gen_loss: 0.1314\n",
      "Epoch 343/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.8561 - gen_loss: 0.1614\n",
      "Epoch 344/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.8457 - gen_loss: 0.1915\n",
      "Epoch 345/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.8362 - gen_loss: 0.2252\n",
      "Epoch 346/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.7766 - gen_loss: 0.3374\n",
      "Epoch 347/512\n",
      "74/74 [==============================] - 83s 1s/step - disc_loss: 0.6110 - gen_loss: 0.2525\n",
      "Epoch 348/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6983 - gen_loss: 0.2578\n",
      "Epoch 349/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.7179 - gen_loss: 0.2666\n",
      "Epoch 350/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.7295 - gen_loss: 0.2807\n",
      "Epoch 351/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.7418 - gen_loss: 0.3002\n",
      "Epoch 352/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.7581 - gen_loss: 0.3346\n",
      "Epoch 353/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.7486 - gen_loss: 0.3627\n",
      "Epoch 354/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.7293 - gen_loss: 0.4055\n",
      "Epoch 355/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6850 - gen_loss: 0.4466\n",
      "Epoch 356/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6399 - gen_loss: 0.4874\n",
      "Epoch 357/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.5989 - gen_loss: 0.5235\n",
      "Epoch 358/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.5725 - gen_loss: 0.5615\n",
      "Epoch 359/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.5419 - gen_loss: 0.5915\n",
      "Epoch 360/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.5319 - gen_loss: 0.6322\n",
      "Epoch 361/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.5164 - gen_loss: 0.6842\n",
      "Epoch 362/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.4936 - gen_loss: 0.7459\n",
      "Epoch 363/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.4663 - gen_loss: 0.8166\n",
      "Epoch 364/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.4340 - gen_loss: 0.9074\n",
      "Epoch 365/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 1.0790 - gen_loss: 1.8666\n",
      "Epoch 366/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.5809 - gen_loss: 1.3558\n",
      "Epoch 367/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.5194 - gen_loss: 0.5363\n",
      "Epoch 368/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.4972 - gen_loss: 0.4109\n",
      "Epoch 369/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.5017 - gen_loss: 0.3359\n",
      "Epoch 370/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.5287 - gen_loss: 0.2886\n",
      "Epoch 371/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.5575 - gen_loss: 0.2526\n",
      "Epoch 372/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.5981 - gen_loss: 0.2307\n",
      "Epoch 373/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6324 - gen_loss: 0.2203\n",
      "Epoch 374/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.6710 - gen_loss: 0.2141\n",
      "Epoch 375/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.7162 - gen_loss: 0.2202\n",
      "Epoch 376/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.7604 - gen_loss: 0.2396\n",
      "Epoch 377/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.8053 - gen_loss: 0.2748\n",
      "Epoch 378/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.7241 - gen_loss: 0.3734\n",
      "Epoch 379/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.4705 - gen_loss: 0.4932\n",
      "Epoch 380/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.3942 - gen_loss: 0.5727\n",
      "Epoch 381/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.3446 - gen_loss: 0.6304\n",
      "Epoch 382/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.3067 - gen_loss: 0.6842\n",
      "Epoch 383/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.2964 - gen_loss: 0.7379\n",
      "Epoch 384/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.2854 - gen_loss: 0.8006\n",
      "Epoch 385/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.2501 - gen_loss: 0.8790\n",
      "Epoch 386/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.2220 - gen_loss: 0.9861\n",
      "Epoch 387/512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.9911 - gen_loss: 0.8216\n",
      "Epoch 388/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 1.2110 - gen_loss: 1.1673\n",
      "Epoch 389/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 1.0749 - gen_loss: 0.8109\n",
      "Epoch 390/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.9919 - gen_loss: 0.7902\n",
      "Epoch 391/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.9260 - gen_loss: 0.8127\n",
      "Epoch 392/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.8908 - gen_loss: 0.8473\n",
      "Epoch 393/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.8514 - gen_loss: 0.8486\n",
      "Epoch 394/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.8149 - gen_loss: 0.8606\n",
      "Epoch 395/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.7929 - gen_loss: 0.8789\n",
      "Epoch 396/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.7717 - gen_loss: 0.9231\n",
      "Epoch 397/512\n",
      "74/74 [==============================] - 82s 1s/step - disc_loss: 0.7569 - gen_loss: 0.9202\n",
      "Epoch 398/512\n",
      "74/74 [==============================] - 83s 1s/step - disc_loss: 0.7478 - gen_loss: 0.8740\n",
      "Epoch 399/512\n",
      "74/74 [==============================] - 83s 1s/step - disc_loss: 0.7306 - gen_loss: 0.8270\n",
      "Epoch 400/512\n",
      " 2/74 [..............................] - ETA: 1:19 - disc_loss: 0.7309 - gen_loss: 0.8395"
     ]
    }
   ],
   "source": [
    "with tf.device('/GPU:1'):\n",
    "    hist = Zelda.fit(train_ds, epochs = 512, callbacks = [ModelMonitor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Zelda.generator.save('..//saves//generator_norm.h5')\n",
    "Zelda.discriminator.save('..//saves//discriminator_norm.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random_latent_vectors = tf.random.normal((50, 64, 1))\n",
    "#generated_images = Zelda.generator(random_latent_vectors)\n",
    "#generated_images *= 255\n",
    "#generated_images.numpy()\n",
    "#for i in range(50):\n",
    "#    img = array_to_img(generated_images[i])\n",
    "#    img.save(os.path.join(\"..\\\\progress_images\", f\"generated_img_{i}.png\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
